---
title: "Linear regression"
author: "Caitlin Simopoulos"
date: '2017-05-26'
output:
  beamer_presentation:
    latex_engine: xelatex
  ioslides_presentation: default
subtitle: McMaster University
header-includes:
- \definecolor{Navy}{RGB}{3, 1, 44}
- \definecolor{Purple}{RGB}{65, 34, 52}
- \definecolor{PrettyGreen}{RGB}{47, 86, 75}
- \definecolor{Pink}{RGB}{255, 105, 120}
- \definecolor{OffWhite}{RGB}{255, 234, 208}
- \colorlet{WhiteOffWhite}{OffWhite!95!white}
- \definecolor{GreyWhite}{RGB}{255,234,208}
- \definecolor{DarkPurple}{RGB}{51, 41, 56}
- \definecolor{LightishPurple}{RGB}{109, 64, 87}
- \definecolor{Pinky}{RGB}{229, 104, 97}
- \definecolor{Robin}{RGB}{172, 229, 223}
- \colorlet{DarkRobin}{Robin!80!black}
- \colorlet{LightNavy}{Navy!50!white}
- \setbeamercolor{title}{fg=DarkPurple}
- \setbeamercolor{frametitle}{fg=white, bg=LightishPurple}
- \setbeamercolor{normal text}{fg=DarkPurple}
- \setbeamercolor{block title}{fg=white,bg=Pinky}
- \setbeamercolor{block body}{fg=black,bg=Pinky!25!white}
- \setbeamercolor{alerted text}{fg=Pinky}
- \setbeamercolor{itemize item}{fg=DarkPurple}
- \setbeamercolor{framesource}{fg=DarkRobin}
- \setbeamercolor{section in toc}{fg=LightNavy}
- \setbeamercolor{footnote}{fg=DarkPurple}
- \setbeamercolor{footnote mark}{fg=DarkPurple}
- \setbeamercolor{myfootlinetext}{fg=DarkPurple}
- \setbeamertemplate{itemize subitem}{\color{DarkRobin}$\blacktriangleright$}
- \setbeamertemplate{itemize subsubitem}{\color{DarkPurple}$\blacktriangleright$}
- \hypersetup{colorlinks,linkcolor=,urlcolor=Pinky}
- \newcommand{\colbegin}{\begin{columns}}
- \newcommand{\colend}{\end{columns}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Let's do some actual analysis!

Now that we've got the hang of some R commands, let's do some real analysis

# Linear regression
- explains the relationship between dependent (Y) and independent (X) variables 
- ``line of best fit'' to data
- a ``simple'' linear regression is equivalent to a correlation
- is a *supervised* machine learning technique
  - the model learns from known data, and can be used to predict
  - sometimes we are looking for trends and don't care about predictions
  
# Linear regression 

linear regression takes the linear equation of:

$$ Y = mX + b $$
But we're going to use it more like:

$$ \hat{y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}x_{i} + \epsilon_{i}$$

where:  
$\hat{y_{i}} =$ predicted response for expeimental unit $i$  
$x_{i} =$ predictor (or independent variable) of experimental unit $i$  
$\hat{\beta_{0}} =$ is expected value when $x_{i} = 0$ (intercept)  
$\hat{\beta_{1}} =$ slope  
$\epsilon =$ some error, because models are never perfect!  

[//]: $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$ are unknown

# $\beta_{0}$ and $\beta_{1}$

$\beta_{0}$ and $\beta_{1}$ are unknown.  
We will estimate these coefficients from known data.
To do this, we need to estimate a line of best fit than minimizes error

# An example: Does head size predict brain size?

\footnotesize
```{r, cache=TRUE}
library(data.table) 
# we've already loaded this package...
head_brain <- fread('http://www.stat.ufl.edu/~winner/data/brainhead.dat')
head(head_brain)
```

# Brain and head size data

Columns are:

1. gender (1 = male, 2 = female)
2. age (1 = 20-46, 2 = 46+)
3. head size ($cm^{3}$)
4. brain weight (g)

```{r, cache=TRUE}
# name columns to reflect what they are 
colnames(head_brain) = c("gender", "age", "head", "brain")
```


# Change data type 
```{r, cache=TRUE}
head_brain$gender <- as.factor(head_brain$gender)
head_brain$age <- as.factor(head_brain$age)
head_brain$head <- as.numeric(head_brain$head)
head_brain$brain <- as.numeric(head_brain$brain)
```


# Plot data

\footnotesize
```{r, cache=TRUE, fig.height=5}
#attach(head_brain)
plot(head_brain$head, head_brain$brain, 
     main="Head size vs brain weight", 
     xlab = "head size (cm^3)", ylab = "brain weight")

```

# How to do a linear regression
\tiny
```{r, cache=TRUE, results="hide", fig.height=5}
set.seed(519)
lm1 = lm(brain ~ head, data=head_brain)
plot(head_brain$head, head_brain$brain,
     main="Head size vs brain weight", 
     xlab = "head size (cm^3)", ylab = "brain weight")
abline(lm1, col="purple")
```

# Summary of the linear regression
[Here's](https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output) a good explanation of the summary in MUCH more detail.

\tiny
```{r cache=TRUE}
summary(lm1)
```


# Summary output gives a lot of information

1. Info about distrubution of residuals (errors)
2. The estimates of our coefficients
3. Standard error of coefficient estimates
    - square root of the variance 
      - $\sqrt{\sigma}^{2}$
3. t-values (coefficient estimates/standard error)
3. $R^{2}$ = (want closer to 1)
4. p-values (yuck!)
    - testing if your coefficient = 0

# Quick residual check

What is a residual?

`Residuals`

`   Min      1Q  Median      3Q     Max 
-175.98  -49.76   -1.76   46.60  242.34`

- Median should be around 0
- 1Q and 3Q should be roughly of same absolute magnitube

# Linear equation with coefficients

- $\beta_{0}$ = intercept = 325.57
- $\beta_{1}$ =  head size coefficient = 0.26

$$\hat{y_{i}} = 325.57 + 0.26x_{i} + \epsilon_{i}$$

\alert{What does this mean in actual words?}

# Use confidence intervals instead of p-values
```{r, cache=TRUE}
confint(lm1, level=0.95) #95% confidence interval
```
# We have more information than just head size!

We also have age range and sex!  
We can use this information to potentially improve our regression by using a multiple linear regression model.

[Here's](https://onlinecourses.science.psu.edu/stat501/node/311) a good tutorial.

# Multiple linear regression

First, let's change out age and sex categories into 0s and 1s.  
(This is the convention in programming)
\footnotesize
```{r, cache=TRUE}
head_brain$gender = as.factor(ifelse(head_brain$gender == 1, 1, 0))
# if head_brain$gender[i] == 1
#     head_brain$gender[i] == 1
# else
#     head_brain$gender[i] == 0
head_brain$age = as.factor(ifelse(head_brain$age == 1, 1, 0))
```

# Does age have an effect on brain size?

We have some options on possible formulas for the linear regression:

* brain ~ head + age
    * $\hat{brain_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}head_{i} + \hat{\beta_{2}}age_{i} + \epsilon_{i}$
* brain ~ head:age 
    * $\hat{brain_{i}} = \hat{\beta_{0}} + \hat{\beta_{2}}age_{i}head_{i} + \epsilon_{i}$
* brain ~ head*age = brain ~ head + age + head x age
    * $\hat{brain_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}head_{i} + \hat{\beta_{2}}age_{i} + \hat{\beta_{2}}age_{i}head_{i} + \epsilon_{i}$
```{r, cache=TRUE}
lm2 = lm(brain ~ head + age, data=head_brain)
lm3 = lm(brain ~ head:age, data=head_brain)
lm4 = lm(brain ~ head * age, data=head_brain)
```

# How can we interpret this?

Interactions mean that slopes is different for each age category and each head size. 

# Evaluate


How can we evaluate these models?


# Do these models meet the asumptions of linear models?
\footnotesize
```{r}
par(mfrow=c(2,2)) 
plot(lm1)
```

# Diagnostic plot

* Residuals vs. Fitted
    * look for residuals randomly distributed around 0.
* Q-Q plot
    * used to look for normality of residuals
    * want the points to follow the diagonal line.
* Scale-Location plot
    * Similar to residuals vs fitted.
* Residuals vs. Leverage plot
    * Was our model driven by outliers? 
        * Cook's distance lines would be visible (they aren't right now) 
        * Data points driving the model would be labeled
        * If this happens, try removing the outliers. 

\footnotesize 
*PS. [http://data.library.virginia.edu/diagnostic-plots/](This website) is helpful for interpretting these plots.*

# Predictions

What if we are given new data, and want to make predictions from our models?

\footnotesize
```{r, cache=TRUE}
new_data = data.frame(gender = as.factor(c(1,1,0,0)), age = as.factor(c(0,1,0,1)), head = as.numeric(c(2265, 999, 2775, 9275)))
print(new_data)
```

# Predictions 
\footnotesize
```{r, cache=TRUE}
lm1_pred <- predict(lm1,newdata=new_data,
                    interval="confidence", level=.95)
lm2_pred <- predict(lm2,newdata=new_data,
                    interval="confidence", level=.95)
lm3_pred <- predict(lm3,newdata=new_data,
                    interval="confidence", level=.95)
lm4_pred<- predict(lm4,newdata=new_data,
                   interval="confidence", level=.95)
```


# Predictions
To see our predictions...

How do they compare?

```{r, eval=FALSE}
print(lm1_pred)
print(lm2_pred)
print(lm3_pred)
print(lm4_pred)
```

# Can we test which model has the best predictions?

How can we do this?

# Acknowledgements
This document was inspired by: [this tutorial](https://rstudio-pubs-static.s3.amazonaws.com/163576_2f7830d4ff104931bca800b4b5eb1ec0.html)

